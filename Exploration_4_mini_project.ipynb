{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incredible-shape",
   "metadata": {},
   "source": [
    "# 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accredited-decline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "    \n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "handled-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0:continue\n",
    "    if sentence[-1] == \":\":continue\n",
    "        \n",
    "    if index > 9:break\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disciplinary-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_sentence(sentence):\n",
    "#     sentence = sentence.lower().strip() # 단어를 소문자로 바꾸고 공백삭제\n",
    "#     sentence = re.sub(r\"([?.!,¿])\", r\"\\1\", sentence) # 특수문자 양쪽에 공백??\n",
    "#     sentence = re.sub(r'[\" \"]', \" \", sentence) # 여러개의 공백은 하나의 공백으로 처리\n",
    "#     sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 문자는 공백으로 처리\n",
    "#     sentence = sentence.strip() # 양쪽 공백 처리\n",
    "#     sentence = \"<start>\" + sentence + '<end>' # 문장 처음에는 <start>, 끝에는 <end> 추가\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "retained-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "functioning-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "warming-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "noticed-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0bfb983190>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 7000,\n",
    "        filters = ' ',\n",
    "        oov_token = \"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "furnished-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fallen-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for index in tokenizer.index_word:\n",
    "    print(index, \":\", tokenizer.index_word[index])\n",
    "    \n",
    "    if index >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "american-globe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:,:-1]\n",
    "tgt_input = tensor[:,1:]\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daily-mention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input)//BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "korean-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "international-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-2.1109100e-04,  1.8291328e-04, -4.7705293e-04, ...,\n",
       "         -4.3738744e-04,  3.0917006e-05, -1.6622123e-04],\n",
       "        [ 6.3367719e-05,  2.5092167e-04, -7.2886981e-04, ...,\n",
       "         -6.3748332e-04, -6.1941086e-05, -1.2380027e-04],\n",
       "        ...,\n",
       "        [ 2.3183892e-03,  1.0343073e-03,  1.2371280e-03, ...,\n",
       "          1.4163012e-03, -1.4375453e-03, -1.9045782e-03],\n",
       "        [ 2.5420196e-03,  1.1397250e-03,  1.5196903e-03, ...,\n",
       "          1.5781472e-03, -1.6570971e-03, -2.0266911e-03],\n",
       "        [ 2.7083051e-03,  1.2338535e-03,  1.7942002e-03, ...,\n",
       "          1.7051207e-03, -1.8662640e-03, -2.1316516e-03]],\n",
       "\n",
       "       [[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-5.7064992e-04,  1.7936347e-04, -5.4900395e-04, ...,\n",
       "         -2.1406755e-04, -2.5462822e-04, -3.2919910e-04],\n",
       "        [-4.2668494e-04,  5.1976516e-05, -3.9852571e-04, ...,\n",
       "          1.3273443e-05, -1.8908861e-04, -5.1836710e-04],\n",
       "        ...,\n",
       "        [ 1.1338829e-03,  1.4541092e-04,  5.4349686e-04, ...,\n",
       "          8.6250732e-04, -1.7264280e-03, -1.3398542e-03],\n",
       "        [ 1.5657501e-03,  3.3889047e-04,  6.5563986e-04, ...,\n",
       "          1.1439942e-03, -1.8200843e-03, -1.6515070e-03],\n",
       "        [ 1.9668832e-03,  5.3399656e-04,  8.3022489e-04, ...,\n",
       "          1.3913693e-03, -1.9057688e-03, -1.8875903e-03]],\n",
       "\n",
       "       [[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-5.7064992e-04,  1.7936347e-04, -5.4900395e-04, ...,\n",
       "         -2.1406755e-04, -2.5462822e-04, -3.2919910e-04],\n",
       "        [-7.2764477e-04,  3.0466370e-04, -4.2619137e-04, ...,\n",
       "         -2.6726632e-04, -3.1139475e-04, -1.6429352e-04],\n",
       "        ...,\n",
       "        [ 2.0916655e-03,  7.4915146e-04,  1.5772270e-03, ...,\n",
       "          1.9582890e-03, -1.7297410e-03, -1.7942864e-03],\n",
       "        [ 2.3332601e-03,  8.6869608e-04,  1.8437938e-03, ...,\n",
       "          2.0644232e-03, -1.9396640e-03, -1.9632482e-03],\n",
       "        [ 2.5186064e-03,  9.8166673e-04,  2.0998768e-03, ...,\n",
       "          2.1398487e-03, -2.1355718e-03, -2.1028738e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-4.6945535e-04,  4.9100717e-04, -3.0518361e-04, ...,\n",
       "         -5.6234200e-04, -3.3581670e-04,  1.6534474e-04],\n",
       "        [-4.0735301e-04,  6.5906672e-04, -4.3087464e-04, ...,\n",
       "         -9.2126022e-04, -6.2615325e-04,  5.9510570e-04],\n",
       "        ...,\n",
       "        [ 2.0444151e-03,  6.4175401e-05,  6.8257487e-04, ...,\n",
       "          7.2982936e-04, -2.0397969e-03, -2.5706789e-03],\n",
       "        [ 2.2590372e-03,  2.9279900e-04,  9.8812091e-04, ...,\n",
       "          1.0035010e-03, -2.1791502e-03, -2.5783654e-03],\n",
       "        [ 2.4291149e-03,  5.1102706e-04,  1.3013822e-03, ...,\n",
       "          1.2329473e-03, -2.3112420e-03, -2.5818178e-03]],\n",
       "\n",
       "       [[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-5.7064992e-04,  1.7936347e-04, -5.4900395e-04, ...,\n",
       "         -2.1406755e-04, -2.5462822e-04, -3.2919910e-04],\n",
       "        [-7.0406846e-04,  3.7253296e-04, -5.0801720e-04, ...,\n",
       "         -3.8605317e-04, -4.6635015e-04, -6.1753683e-04],\n",
       "        ...,\n",
       "        [ 2.7664574e-03,  8.6872483e-04,  1.7506309e-03, ...,\n",
       "          1.9028041e-03, -1.9933886e-03, -2.4907608e-03],\n",
       "        [ 2.9083372e-03,  1.0174199e-03,  2.0282075e-03, ...,\n",
       "          2.0072146e-03, -2.1669730e-03, -2.5469249e-03],\n",
       "        [ 3.0040306e-03,  1.1422033e-03,  2.2900910e-03, ...,\n",
       "          2.0825148e-03, -2.3312694e-03, -2.5891960e-03]],\n",
       "\n",
       "       [[-3.5814825e-04,  1.9253716e-04, -2.5566388e-04, ...,\n",
       "         -2.8332704e-04, -5.3213895e-05, -8.1370636e-06],\n",
       "        [-3.5390243e-04,  2.4074424e-04, -2.6193410e-04, ...,\n",
       "         -4.6050115e-04, -4.3576907e-05, -1.2638169e-05],\n",
       "        [-2.2271533e-04, -2.7336416e-04, -5.1550515e-04, ...,\n",
       "         -7.4219849e-04,  4.3398770e-05, -6.9520211e-05],\n",
       "        ...,\n",
       "        [ 1.6935172e-03,  1.3145581e-03,  1.1441116e-03, ...,\n",
       "          1.0960313e-03, -1.7372720e-03, -2.3000913e-03],\n",
       "        [ 2.0219360e-03,  1.3762704e-03,  1.3642281e-03, ...,\n",
       "          1.3481994e-03, -1.9126536e-03, -2.3391149e-03],\n",
       "        [ 2.2979244e-03,  1.4374009e-03,  1.5937580e-03, ...,\n",
       "          1.5576573e-03, -2.0783427e-03, -2.3691582e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1):break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "flying-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fantastic-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 35s 354ms/step - loss: 4.3813\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 33s 356ms/step - loss: 2.8126\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 33s 357ms/step - loss: 2.6950\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 33s 358ms/step - loss: 2.5887\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 33s 358ms/step - loss: 2.5210\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 34s 362ms/step - loss: 2.4649\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 34s 363ms/step - loss: 2.4036\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 34s 363ms/step - loss: 2.3388\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 34s 360ms/step - loss: 2.2902\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 34s 361ms/step - loss: 2.2371\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 34s 360ms/step - loss: 2.1763\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 35s 380ms/step - loss: 2.1256\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 34s 361ms/step - loss: 2.0765\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 35s 380ms/step - loss: 2.0160\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 41s 442ms/step - loss: 1.9666\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 42s 452ms/step - loss: 1.9128\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 47s 509ms/step - loss: 1.8534\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 35s 376ms/step - loss: 1.7930\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 35s 372ms/step - loss: 1.7433\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 33s 360ms/step - loss: 1.6863\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 33s 355ms/step - loss: 1.6271\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 33s 355ms/step - loss: 1.5721\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 33s 357ms/step - loss: 1.5181\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.4639\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 33s 360ms/step - loss: 1.4029\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.3483\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.2810\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.2300\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.1703\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 33s 359ms/step - loss: 1.1119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0bfb580590>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction = 'none'\n",
    ")\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-endorsement",
   "metadata": {},
   "source": [
    "# 미니 프로젝트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ahead-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기 : 187088\n",
      "Examples :\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel//lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file,'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print('데이터 크기 :', len(raw_corpus))\n",
    "print('Examples :\\n', raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defined-university",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Hook]',\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face',\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face [Verse 1]',\n",
       " \"Somethin' ain't right when we talkin'\",\n",
       " \"Somethin' ain't right when we talkin'\",\n",
       " \"Look like you hidin' your problems\",\n",
       " 'Really you never was solid',\n",
       " 'No, you can\\'t \"son\" me',\n",
       " \"You won't never get to run me\",\n",
       " 'Just when shit look out of reach',\n",
       " 'I reach back like one, three',\n",
       " 'Like one, three, yeah [Pre-Hook]',\n",
       " \"That's when they smile in my face\",\n",
       " 'Whole time they wanna take my place',\n",
       " 'Whole time they wanna take my place',\n",
       " 'Whole time they wanna take my place',\n",
       " 'Yeah, I know they wanna take my place',\n",
       " 'I can tell that love is fake',\n",
       " \"I don't trust a word you say\",\n",
       " 'How you wanna clique up after your mistakes?',\n",
       " \"Look you in the face, and it's just not the same [Hook]\",\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face',\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face [Verse 2]',\n",
       " 'Yeah, straight up to my face, tryna play it safe',\n",
       " 'Vibe switch like night and day',\n",
       " 'I can see it, like, right away',\n",
       " 'I came up, you changed up',\n",
       " 'I caught that whole play',\n",
       " 'Since, things never been the same [Pre-Hook]',\n",
       " \"That's when they smile in my face\",\n",
       " 'Whole time they wanna take my place',\n",
       " 'Whole time they wanna take my place',\n",
       " 'Whole time they wanna take my place',\n",
       " 'Yeah, I know they wanna take my place',\n",
       " 'I can tell that love is fake (I can tell that love is fake)',\n",
       " \"I don't trust a word you say (I don't trust a word)\",\n",
       " 'How you wanna clique up after your mistakes?',\n",
       " \"(That's just what I heard)\",\n",
       " \"Look you in the face, and it's just not the same [Hook]\",\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face',\n",
       " \"I've been down so long, it look like up to me\",\n",
       " 'They look up to me',\n",
       " \"I got fake people showin' fake love to me\",\n",
       " 'Straight up to my face, straight up to my face [Outro]',\n",
       " 'Skrrt [Intro: Moodymann]',\n",
       " \"Hold on, hold on, fuck that. Fuck that shit. Hold on, I got to start this mothafuckin' record over again, wait a minute. Fuck that shit. Still on this mothafuckin' record. I'ma play this mothafucka for y'all. Aye, y'all get some more drinks goin' on, I'll sound a whole lot better. [Verse 1]\",\n",
       " \"Listen, seein' you got ritualistic\",\n",
       " \"Cleansin' my soul of addiction for now\",\n",
       " \"'Cause I'm fallin' apart, yeah\",\n",
       " 'Tension between us just like picket fences',\n",
       " \"You got issues that I won't mention for now\",\n",
       " \"'Cause we're fallin' apart [Chorus]\",\n",
       " 'Passionate from miles away',\n",
       " 'Passive with the things you say',\n",
       " \"Passin' up on my old ways\",\n",
       " \"I can't blame you, no, no\",\n",
       " 'Passionate from miles away',\n",
       " 'Passive with the things you say',\n",
       " \"Passin' up on my old ways\",\n",
       " \"I can't blame you, no, no [Verse 2]\",\n",
       " \"Listen, hard at buildin' trust from a distance\",\n",
       " 'I think we should rule out commitment for now',\n",
       " \"'Cause we're fallin' apart\",\n",
       " \"Leavin', you're just doin' that to get even\",\n",
       " \"Don't pick up the pieces, just leave it for now\",\n",
       " \"They keep fallin' apart [Chorus]\",\n",
       " 'Passionate from miles away',\n",
       " 'Passive with the things you say',\n",
       " \"Passin' up on my old ways\",\n",
       " \"I can't blame you, no, no\",\n",
       " 'Passionate from miles away',\n",
       " 'Passive with the things you say',\n",
       " \"Passin' up on my old ways\",\n",
       " \"I can't blame you, no, no [Outro]\",\n",
       " \"Tryin' to think of the right thing to say [Hook: Jorja Smith]\",\n",
       " \"These things'll fall down\",\n",
       " \"But you'll pick 'em up\",\n",
       " \"You're still here to touch the ground\",\n",
       " \"Don't worry, I swear\",\n",
       " \"I know you're trying to help me [Verse: Drake]\",\n",
       " 'Tryna stay light on my toes',\n",
       " 'Just ran a light in a Rolls',\n",
       " \"Told me I'm lookin' exhausted\",\n",
       " 'You hit it right on the nose',\n",
       " \"I'm tired of all of these niggas\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if index > 9: break\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "closing-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> hold on , hold on , fuck that . fuck that shit . hold on , i got to start this mothafuckin  record over again , wait a minute . fuck that shit . still on this mothafuckin  record . i ma play this mothafucka for y all . aye , y all get some more drinks goin  on , i ll sound a whole lot better . <end>\n"
     ]
    }
   ],
   "source": [
    "# 문장 전처리하기\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub('\\[.+?\\]', '', sentence)\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[ ]+', \" \", sentence)\n",
    "    sentence = re.sub(r'[^a-zA-Z?.!,¿]', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"Hold on, hold on, fuck that. Fuck that shit. Hold on, I got to start this mothafuckin' record over again, wait a minute. Fuck that shit. Still on this mothafuckin' record. I'ma play this mothafucka for y'all. Aye, y'all get some more drinks goin' on, I'll sound a whole lot better. [Verse 1]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "resistant-credits",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>  <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> somethin  ain t right when we talkin <end>',\n",
       " '<start> somethin  ain t right when we talkin <end>',\n",
       " '<start> look like you hidin  your problems <end>',\n",
       " '<start> really you never was solid <end>',\n",
       " '<start> no , you can t  son  me <end>',\n",
       " '<start> you won t never get to run me <end>',\n",
       " '<start> just when shit look out of reach <end>',\n",
       " '<start> i reach back like one , three <end>',\n",
       " '<start> like one , three , yeah <end>',\n",
       " '<start> that s when they smile in my face <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> yeah , i know they wanna take my place <end>',\n",
       " '<start> i can tell that love is fake <end>',\n",
       " '<start> i don t trust a word you say <end>',\n",
       " '<start> how you wanna clique up after your mistakes ? <end>',\n",
       " '<start> look you in the face , and it s just not the same <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> yeah , straight up to my face , tryna play it safe <end>',\n",
       " '<start> vibe switch like night and day <end>',\n",
       " '<start> i can see it , like , right away <end>',\n",
       " '<start> i came up , you changed up <end>',\n",
       " '<start> i caught that whole play <end>',\n",
       " '<start> since , things never been the same <end>',\n",
       " '<start> that s when they smile in my face <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> whole time they wanna take my place <end>',\n",
       " '<start> yeah , i know they wanna take my place <end>',\n",
       " '<start> how you wanna clique up after your mistakes ? <end>',\n",
       " '<start> that s just what i heard <end>',\n",
       " '<start> look you in the face , and it s just not the same <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> i ve been down so long , it look like up to me <end>',\n",
       " '<start> they look up to me <end>',\n",
       " '<start> i got fake people showin  fake love to me <end>',\n",
       " '<start> straight up to my face , straight up to my face <end>',\n",
       " '<start> skrrt <end>',\n",
       " '<start> listen , seein  you got ritualistic <end>',\n",
       " '<start> cleansin  my soul of addiction for now <end>',\n",
       " '<start> cause i m fallin  apart , yeah <end>',\n",
       " '<start> tension between us just like picket fences <end>',\n",
       " '<start> you got issues that i won t mention for now <end>',\n",
       " '<start> cause we re fallin  apart <end>',\n",
       " '<start> passionate from miles away <end>',\n",
       " '<start> passive with the things you say <end>',\n",
       " '<start> passin  up on my old ways <end>',\n",
       " '<start> i can t blame you , no , no <end>',\n",
       " '<start> passionate from miles away <end>',\n",
       " '<start> passive with the things you say <end>',\n",
       " '<start> passin  up on my old ways <end>',\n",
       " '<start> i can t blame you , no , no <end>',\n",
       " '<start> listen , hard at buildin  trust from a distance <end>',\n",
       " '<start> i think we should rule out commitment for now <end>',\n",
       " '<start> cause we re fallin  apart <end>',\n",
       " '<start> leavin  , you re just doin  that to get even <end>',\n",
       " '<start> don t pick up the pieces , just leave it for now <end>',\n",
       " '<start> they keep fallin  apart <end>',\n",
       " '<start> passionate from miles away <end>',\n",
       " '<start> passive with the things you say <end>',\n",
       " '<start> passin  up on my old ways <end>',\n",
       " '<start> i can t blame you , no , no <end>',\n",
       " '<start> passionate from miles away <end>',\n",
       " '<start> passive with the things you say <end>',\n",
       " '<start> passin  up on my old ways <end>',\n",
       " '<start> i can t blame you , no , no <end>',\n",
       " '<start> tryin  to think of the right thing to say <end>',\n",
       " '<start> these things ll fall down <end>',\n",
       " '<start> but you ll pick  em up <end>',\n",
       " '<start> you re still here to touch the ground <end>',\n",
       " '<start> don t worry , i swear <end>',\n",
       " '<start> i know you re trying to help me <end>',\n",
       " '<start> tryna stay light on my toes <end>',\n",
       " '<start> just ran a light in a rolls <end>',\n",
       " '<start> told me i m lookin  exhausted <end>',\n",
       " '<start> you hit it right on the nose <end>',\n",
       " '<start> i m tired of all of these niggas <end>',\n",
       " '<start> i m tired of all of these hoes <end>',\n",
       " '<start> worried  bout takin  my lane <end>',\n",
       " '<start> they ain t even got on my road <end>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    \n",
    "    if len(sentence) == 0: continue\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    if len(text_to_word_sequence(preprocessed_sentence)) >= 15: # 토큰의 길이가 15가 넘는 문장은 빼기\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "\n",
    "    \n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "external-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    3    0 ...    0    0    0]\n",
      " [   2    5   97 ...    0    0    0]\n",
      " [   2   40  132 ...    0    0    0]\n",
      " ...\n",
      " [   2  202    3 ...    0    0    0]\n",
      " [   2  424    9 ...    0    0    0]\n",
      " [   2    9 1564 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fb93fef11d0>\n"
     ]
    }
   ],
   "source": [
    "# 텐서화 시키기\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000,\n",
    "        filters = '',\n",
    "        oov_token = '<unk>'\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "assisted-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2   5  97 108  59  31 166   4  11 132  23  29  10  12   3   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  2  40 132  29  10  12   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eligible-commons",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  <unk>\n",
      "2  :  <start>\n",
      "3  :  <end>\n",
      "4  :  ,\n",
      "5  :  i\n",
      "6  :  the\n",
      "7  :  you\n",
      "8  :  and\n",
      "9  :  a\n",
      "10  :  to\n"
     ]
    }
   ],
   "source": [
    "# 구축된 단어사전 확인해보기\n",
    "for index in tokenizer.index_word:\n",
    "    print(index, \" : \", tokenizer.index_word[index])\n",
    "    \n",
    "    if index >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governing-parliament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5  97 108  59  31 166   4  11 132  23  29  10  12   3   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  5  97 108  59  31 166   4  11 132  23  29  10  12   3   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "155622"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input 데이터와 output 데이터 만들기\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[1])\n",
    "print(tgt_input[1])\n",
    "len(src_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "biological-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 31), (256, 31)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_size = len(src_input)\n",
    "batch_size = 256\n",
    "steps_per_epoch = len(src_input)//batch_size\n",
    "\n",
    "vocab_size = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder = True)\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dated-campbell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124497, 31)\n",
      "Target Train: (124497, 31)\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터와 평가데이터로 분리하기\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "drawn-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공지능 학습시키기\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incoming-auckland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 31, 12001), dtype=float32, numpy=\n",
       "array([[[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [ 4.90466657e-04, -3.26571142e-04,  1.35671085e-04, ...,\n",
       "          5.37909800e-05, -1.41416662e-04, -3.19969346e-04],\n",
       "        [ 8.55844351e-04, -4.91262123e-04,  3.33294083e-05, ...,\n",
       "         -1.22903060e-04, -1.99335816e-04, -3.70199559e-04],\n",
       "        ...,\n",
       "        [ 4.21123859e-03, -4.92468430e-03, -5.23592392e-03, ...,\n",
       "         -2.19860769e-04,  3.16735078e-03, -3.57267307e-03],\n",
       "        [ 4.33726236e-03, -5.04222233e-03, -5.38091827e-03, ...,\n",
       "         -1.79692724e-04,  3.29125882e-03, -3.64975329e-03],\n",
       "        [ 4.45285439e-03, -5.14252204e-03, -5.50540304e-03, ...,\n",
       "         -1.40434175e-04,  3.40141193e-03, -3.71687417e-03]],\n",
       "\n",
       "       [[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [-5.77989194e-05, -9.07840222e-05, -1.45869984e-04, ...,\n",
       "         -5.71974961e-05, -1.20708763e-04, -5.66010363e-04],\n",
       "        [ 1.96982382e-05, -2.71302706e-04, -3.46116954e-04, ...,\n",
       "         -6.10169700e-05, -1.68931030e-04, -5.93328790e-04],\n",
       "        ...,\n",
       "        [ 4.75159241e-03, -5.27674658e-03, -5.71632758e-03, ...,\n",
       "         -2.20885922e-05,  3.54969664e-03, -3.75858787e-03],\n",
       "        [ 4.82896389e-03, -5.34828752e-03, -5.79352817e-03, ...,\n",
       "          3.86239026e-06,  3.63384048e-03, -3.81929614e-03],\n",
       "        [ 4.89728292e-03, -5.40643884e-03, -5.85763948e-03, ...,\n",
       "          2.92274453e-05,  3.70702799e-03, -3.86933517e-03]],\n",
       "\n",
       "       [[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [ 2.63451744e-04, -2.13218285e-04, -8.62770394e-05, ...,\n",
       "          1.85046476e-04, -2.00055321e-04, -8.93661781e-05],\n",
       "        [ 3.71466827e-04, -4.71880980e-04, -1.89770770e-04, ...,\n",
       "          1.18800803e-04, -5.95192425e-04, -1.15267707e-04],\n",
       "        ...,\n",
       "        [ 5.12512308e-03, -5.52499620e-03, -5.97482268e-03, ...,\n",
       "          1.24325423e-04,  3.88137228e-03, -3.97259230e-03],\n",
       "        [ 5.15436288e-03, -5.54227829e-03, -6.00407785e-03, ...,\n",
       "          1.41878365e-04,  3.91960656e-03, -3.98949394e-03],\n",
       "        [ 5.17865689e-03, -5.55550447e-03, -6.02838863e-03, ...,\n",
       "          1.57567265e-04,  3.95242264e-03, -4.00348660e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [ 1.47696526e-04, -4.56070447e-05, -1.33781970e-04, ...,\n",
       "          7.24656275e-05,  1.05710955e-04, -3.93045513e-04],\n",
       "        [-4.13754424e-05, -2.02111303e-04, -1.43635858e-04, ...,\n",
       "         -1.57878472e-04,  1.53608155e-04, -3.56923119e-04],\n",
       "        ...,\n",
       "        [ 4.88560461e-03, -5.35034388e-03, -5.96662425e-03, ...,\n",
       "          5.97138205e-05,  3.70550831e-03, -3.97933507e-03],\n",
       "        [ 4.94653452e-03, -5.39897941e-03, -6.00486202e-03, ...,\n",
       "          8.66675764e-05,  3.76489200e-03, -3.99849005e-03],\n",
       "        [ 4.99963667e-03, -5.43909753e-03, -6.03547273e-03, ...,\n",
       "          1.10692832e-04,  3.81698995e-03, -4.01399797e-03]],\n",
       "\n",
       "       [[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [ 5.60029948e-05, -5.94496851e-05,  1.60289856e-04, ...,\n",
       "         -1.35689595e-04, -1.60811687e-04, -2.50027952e-04],\n",
       "        [ 2.65778857e-04, -1.78021946e-04,  1.15206785e-04, ...,\n",
       "         -3.84826621e-04, -2.10825267e-04, -1.55715810e-04],\n",
       "        ...,\n",
       "        [ 4.96060168e-03, -5.43946214e-03, -5.88062219e-03, ...,\n",
       "          1.78395803e-05,  3.69611755e-03, -3.95428436e-03],\n",
       "        [ 5.01377322e-03, -5.47605520e-03, -5.92424488e-03, ...,\n",
       "          4.59190414e-05,  3.76090966e-03, -3.97959724e-03],\n",
       "        [ 5.05961524e-03, -5.50510315e-03, -5.96079929e-03, ...,\n",
       "          7.16244322e-05,  3.81678320e-03, -3.99990799e-03]],\n",
       "\n",
       "       [[ 9.91955967e-05, -5.00889219e-05,  1.82585900e-05, ...,\n",
       "          1.78373084e-05, -3.50100299e-06, -1.92250183e-04],\n",
       "        [ 2.61208712e-04, -4.25717008e-06, -2.43906121e-04, ...,\n",
       "          2.48020078e-04, -4.69719598e-05, -4.35490976e-04],\n",
       "        [ 3.89694265e-04, -6.52306553e-05, -8.96240468e-04, ...,\n",
       "          1.49573039e-04, -1.83363023e-04, -3.01933906e-04],\n",
       "        ...,\n",
       "        [ 4.93766135e-03, -5.35215857e-03, -5.84023399e-03, ...,\n",
       "         -4.97145593e-05,  3.66842723e-03, -3.91897932e-03],\n",
       "        [ 4.98987688e-03, -5.40512148e-03, -5.89227211e-03, ...,\n",
       "         -8.67274684e-06,  3.73270176e-03, -3.94804776e-03],\n",
       "        [ 5.03567234e-03, -5.44769922e-03, -5.93589386e-03, ...,\n",
       "          2.76070095e-05,  3.78876249e-03, -3.97187518e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1):break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regional-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sporting-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "607/607 [==============================] - 413s 677ms/step - loss: 2.0418\n",
      "Epoch 2/10\n",
      "607/607 [==============================] - 414s 682ms/step - loss: 1.3654\n",
      "Epoch 3/10\n",
      "607/607 [==============================] - 414s 682ms/step - loss: 1.2770\n",
      "Epoch 4/10\n",
      "607/607 [==============================] - 413s 680ms/step - loss: 1.2102\n",
      "Epoch 5/10\n",
      "607/607 [==============================] - 413s 680ms/step - loss: 1.1556\n",
      "Epoch 6/10\n",
      "607/607 [==============================] - 409s 673ms/step - loss: 1.1058\n",
      "Epoch 7/10\n",
      "607/607 [==============================] - 420s 692ms/step - loss: 1.0552\n",
      "Epoch 8/10\n",
      "607/607 [==============================] - 407s 669ms/step - loss: 1.0133\n",
      "Epoch 9/10\n",
      "607/607 [==============================] - 407s 670ms/step - loss: 0.9714\n",
      "Epoch 10/10\n",
      "607/607 [==============================] - 406s 669ms/step - loss: 0.9374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb9304ca190>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction = 'none'\n",
    ")\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "returning-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "determined-representation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like the way how you re kissin me <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-wrestling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
